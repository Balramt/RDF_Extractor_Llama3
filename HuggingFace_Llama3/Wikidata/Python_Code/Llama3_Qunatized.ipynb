{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c574689e-67af-453d-82b3-3ed0c4a63784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should print True if CUDA is available\n",
    "print(torch.cuda.current_device())  # Should print the current GPU device index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc4c66d-2174-4f54-b522-429ed5c0b08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3cf3885-6d10-4466-8955-3075aedb1864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 10 19:32:01 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1080 Ti     On  |   00000000:81:00.0 Off |                  N/A |\n",
      "| 20%   47C    P5             24W /  250W |       3MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44b68d3b-ce49-4074-a313-cb7c462b12ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: kill: (2965937) - No such process\n"
     ]
    }
   ],
   "source": [
    "!kill -9 2965937"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e2b46a-60ef-4f2f-9d9b-ff942a6d64de",
   "metadata": {},
   "source": [
    "# FOR QUANTATIZED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c626893-a4b8-4673-bd87-084a63a257d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d296279-79cf-456e-80eb-4d285c8198b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc453bc3-8807-4adf-bb81-37c02fd9c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac32aa6e-8837-4467-a6e0-37d121ba7350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 19:32:08.818710: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-10 19:32:08.844179: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-10 19:32:08.852016: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-10 19:32:08.871737: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-10 19:32:09.899315: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "783f489c-78a7-460a-8048-2527295aed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2c3e151-a51f-4550-a5f2-3a28b7171923",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "518fb1e2-e165-4302-b702-98b53a8a74a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78e83694-82dd-4526-a17e-bc4b3343673e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04adb17b388d42b59dd1f2e5ceac19cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model with 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_4bit=True,  # Enable 4-bit quantization\n",
    "    device_map=\"auto\",  # Automatically place model layers on GPU\n",
    "    torch_dtype=torch.float16\n",
    "    #quantization_config=bnb.BnbQuantizationConfig(load_in_4bit=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "425a5e20-8589-406e-9ad1-b7cb57a9da43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text-generation pipeline\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer\n",
    "    #device=0  # Use GPU 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "824978fe-fadb-47f7-b38f-405838fd955e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check if the model is on GPU or CPU\n",
    "for param in model.parameters():\n",
    "    print(f\"Model is on: {param.device}\")\n",
    "    break  # Check one parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34123428-f2e7-4ec0-88b7-ed1cd384dc37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf7b5c8d-2a11-400a-b0aa-3512541e1de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "227c407f-7514-48a1-b84d-232421073bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cae869ff-baac-43f7-96c8-df5a511a3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "358a836a-3f93-4311-af33-965fad62c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompts_data(filepath):\n",
    "    with jsonlines.open(filepath) as reader:\n",
    "        return list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b9a25431-c008-43c5-b426-64acef98d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompt_info(item):\n",
    "    prompt_id = item.get('id')\n",
    "    prompt_text = item.get('prompt')\n",
    "    print(\"prompt_text\",prompt_text)\n",
    "    return prompt_id, prompt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a70f15d0-6e8f-472a-aeca-9b1b39d5b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'text_generator' is already defined somewhere in your environment\n",
    "def generate_text(prompt_text, max_length=500, num_return_sequences=1):\n",
    "    response = text_generator(prompt_text, max_length=max_length, num_return_sequences=num_return_sequences)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4edbefb9-a8b9-4b67-8dd7-3f0a913bf3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_output(response):\n",
    "    print(\"response\",response)\n",
    "    if response and len(response) > 0:\n",
    "        generated_text = response[0].get('generated_text', '')\n",
    "        match = re.search(r'Test Output:\\s*(.*?)\\s*(?:\\n|$)', generated_text, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    return 'Output not found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35befec8-b3e5-4e9b-8b83-886c5c714409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "18baec05-7db9-4644-a808-f8fd461de7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_model_output(model_output):\n",
    "    # Initialize an empty list to store triples\n",
    "    triples = []\n",
    "    \n",
    "    # Remove leading/trailing whitespace and split the input data into lines\n",
    "    lines = [line.strip() for line in model_output.strip().split('\\n')]\n",
    "    \n",
    "    # Define the regex pattern to match triples in the format: relation(subject, object)\n",
    "    pattern = re.compile(r'(.+?)\\s*\\(([^,]+),\\s*([^)]+)\\)')\n",
    "    \n",
    "    for line in lines:\n",
    "        # Find all matches for the pattern in the line\n",
    "        matches = pattern.findall(line)\n",
    "        \n",
    "        for match in matches:\n",
    "            relation, subject, obj = match\n",
    "            # Clean up subject and object values\n",
    "            subject = subject.strip()\n",
    "            obj = obj.strip()\n",
    "            \n",
    "            # Append the extracted triple to the list\n",
    "            triples.append({\"sub\": subject, \"rel\": relation, \"obj\": obj})\n",
    "    \n",
    "    # Return the list of triples\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ee29d69c-06d0-436d-a1bc-ddf10f853353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JSONL_FILEPATH = 'ont_1_movie_prompts.jsonl'\n",
    "#output_filepath='LLM_Response.jsonl'\n",
    "JSONL_FILEPATH = 'Wikidata/Input_Prompts/ont_4_book_prompts.jsonl'\n",
    "output_filepath='Wikidata/Response/ont_4_book_llm_response.jsonl'\n",
    "\n",
    "#'Wikidata/Evaluation_Statistics/ont_6_computer_llm_stats.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "33ec600e-0195-41c0-abe1-358b4d71a021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(filepath, output_filepath, num_prompts=4):\n",
    "    prompts_data = load_prompts_data(filepath)\n",
    "    processed_data = []\n",
    "    for i in range(min(num_prompts, len(prompts_data))):\n",
    "        item = prompts_data[i]\n",
    "        prompt_id, prompt_text = extract_prompt_info(item)\n",
    "        response = generate_text(prompt_text)\n",
    "        test_output = extract_test_output(response)\n",
    "        \n",
    "        # Debugging: Print the test output to ensure it's correctly extracted\n",
    "        print(f\"Test Output for ID {prompt_id}: {test_output}\")\n",
    "        \n",
    "        # Parse the test output into triples\n",
    "        triples = parse_model_output(test_output)\n",
    "        \n",
    "        # Debugging: Print the parsed triples to ensure they're correct\n",
    "        print(f\"Parsed Triples for ID {prompt_id}: {triples}\")\n",
    "        \n",
    "        processed_entry = {\n",
    "            \"id\": prompt_id,\n",
    "            \"triples\": triples\n",
    "        }\n",
    "        \n",
    "        processed_data.append(processed_entry)\n",
    "    \n",
    "    # Save the processed data into a new jsonl file\n",
    "    save_triples_to_jsonl(processed_data, output_filepath)\n",
    "    print(f\"Processed triples saved to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "36530dea-531f-4f62-bd18-39d8e57b11c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_text \n",
      "Given the following ontology and sentences, please extract the triples from the sentence according to the relations in the ontology. In the output, only include the triples in the given output format.\n",
      "CONTEXT:\n",
      "Ontology Concepts: book, literary work, author, publisher, International Standard Book Number, library, calender date, film, fictional character, writer, scientific journal, article, human, literary genre, publication, trade magazine, language, intellectual work, country, territory,\n",
      "Ontology Relations: illustrator(,human), followed_by(,), publication_date(book,), author(book,human), publisher(book,publisher), characters(literary work,fictional character), editor(trade magazine,human), place_of_publication(,), narrative_location(literary work,territory), genre(literary work,literary genre), language_of_work_or_name(literary work,language), depicts(,)\n",
      "\n",
      "Example Sentence: The Foundling and Other Tales of Prydain is a collection of short high fantasy stories for children by Lloyd Alexander and illustrator Margot Zemach.\n",
      "Example Output: illustrator(The Foundling and Other Tales of Prydain,Margot Zemach)\n",
      "\n",
      "Test Sentence: Peter and the Piskies: Cornish Folk and Fairy Tales is a 1958 anthology of 34 fairy tales from Cornwall that have been collected and retold by Ruth Manning-Sanders and illustrated by Raymond Briggs.\n",
      "Test Output: \n",
      "response [{'generated_text': '\\nGiven the following ontology and sentences, please extract the triples from the sentence according to the relations in the ontology. In the output, only include the triples in the given output format.\\nCONTEXT:\\nOntology Concepts: book, literary work, author, publisher, International Standard Book Number, library, calender date, film, fictional character, writer, scientific journal, article, human, literary genre, publication, trade magazine, language, intellectual work, country, territory,\\nOntology Relations: illustrator(,human), followed_by(,), publication_date(book,), author(book,human), publisher(book,publisher), characters(literary work,fictional character), editor(trade magazine,human), place_of_publication(,), narrative_location(literary work,territory), genre(literary work,literary genre), language_of_work_or_name(literary work,language), depicts(,)\\n\\nExample Sentence: The Foundling and Other Tales of Prydain is a collection of short high fantasy stories for children by Lloyd Alexander and illustrator Margot Zemach.\\nExample Output: illustrator(The Foundling and Other Tales of Prydain,Margot Zemach)\\n\\nTest Sentence: Peter and the Piskies: Cornish Folk and Fairy Tales is a 1958 anthology of 34 fairy tales from Cornwall that have been collected and retold by Ruth Manning-Sanders and illustrated by Raymond Briggs.\\nTest Output:  illustrator(Peter and the Piskies: Cornish Folk and Fairy Tales,Raymond Briggs)'}]\n",
      "Test Output for ID ont_4_book_test_1: illustrator(Peter and the Piskies: Cornish Folk and Fairy Tales,Raymond Briggs)\n",
      "Parsed Triples for ID ont_4_book_test_1: [{'sub': 'Peter and the Piskies: Cornish Folk and Fairy Tales', 'rel': 'illustrator', 'obj': 'Raymond Briggs'}]\n",
      "Processed triples saved to Wikidata/Response/ont_4_book_llm_response.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Replace 'your_input_filepath.jsonl' with your actual input file path\n",
    "# Replace 'your_output_filepath.jsonl' with your desired output file path\n",
    "main(JSONL_FILEPATH, output_filepath, num_prompts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd2afc-a2a3-4423-b363-319ca1507bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57923fce-4f07-4815-af97-1773fdf2cfe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e9afb-4ab2-4c1d-bb7e-0aa6832f2380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f947cf2-8c52-4d70-ae9d-1492e6edbbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcedb68-5fa3-49a4-863b-340d661cb900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d65e6c-13fe-461d-94f0-944df8a3d05d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef70c74-56b6-4688-80f3-1e3956285244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c68964-81cb-4a04-b3ca-f366d95ed013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d039b9-58a3-4c8c-8b24-eb6c47d4c3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470ea5bf-f6ab-4407-8623-daadfeae5f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe3e9d-5c07-4455-9b67-d9b553c5145c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2bad6a-361d-49e5-9f1d-d2234070f38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d5491e-c250-45d6-a46e-34c91d35c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "cefb5510-fc5e-49ce-93e1-5d57c7539ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a JSONL file.\n",
    "    \n",
    "    :param file_path: Path to the JSONL file.\n",
    "    :return: A list of dictionaries representing each line of the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "914acf67-2a76-49a0-90e7-d593513de9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "ground_truth = load_jsonl('ont_1_movie_prompts.jsonl')\n",
    "system_predicted = load_jsonl('LLM_Response.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "c5f73d76-169c-49a3-a603-1b630167ecf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalized System Predicted Triples:\n",
      "ID: ont_1_movie_test_1\n",
      "Original Triple: {'sub': 'Bleach: Hell Verse', 'rel': 'director', 'obj': 'Noriyuki Abe'}\n",
      "Normalized Triple: bleachhellversedirectornoriyukiabe\n",
      "ID: ont_1_movie_test_2\n",
      "Original Triple: {'sub': 'Keyboard Cat', 'rel': 'director', 'obj': 'Charlie Schmidt'}\n",
      "Normalized Triple: keyboardcatdirectorcharlieschmidt\n",
      "ID: ont_1_movie_test_3\n",
      "Original Triple: {'sub': 'Tenchi Forever! The Movie', 'rel': 'director', 'obj': 'Mitsuko Kase'}\n",
      "Normalized Triple: tenchiforeverthemoviedirectormitsukokase\n",
      "Original Triple: {'sub': 'Tenchi Forever! The Movie', 'rel': ',director', 'obj': 'Takashi Imanishi'}\n",
      "Normalized Triple: tenchiforeverthemoviedirectortakashiimanishi\n",
      "ID: ont_1_movie_test_4\n"
     ]
    }
   ],
   "source": [
    "# Normalize and print system predicted triples\n",
    "print(\"\\nNormalized System Predicted Triples:\")\n",
    "for entry in system_predicted:\n",
    "    print(f\"ID: {entry['id']}\")\n",
    "    for triple in entry['triples']:\n",
    "        norm_triple = normalize_triple(triple['sub'], triple['rel'], triple['obj'])\n",
    "        print(f\"Original Triple: {triple}\")\n",
    "        print(f\"Normalized Triple: {norm_triple}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ec227-3d1f-4bbc-b0e8-35a53d3e265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and print ground truth triples\n",
    "print(\"Normalized Ground Truth Triples:\")\n",
    "for entry in ground_truth:\n",
    "    print(f\"ID: {entry['id']}\")\n",
    "    for triple in entry['triples']:\n",
    "        norm_triple = normalize_triple(triple['sub'], triple['rel'], triple['obj'])\n",
    "        print(f\"Original Triple: {triple}\")\n",
    "        print(f\"Normalized Triple: {norm_triple}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "02f8a36c-8907-478d-a2a4-92373eddfd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(ground_truth, system_predicted):\n",
    "    \"\"\"\n",
    "    Calculate the precision of the system-predicted triples against the ground truth.\n",
    "    \n",
    "    :param ground_truth: List of ground truth entries.\n",
    "    :param system_predicted: List of system-predicted entries.\n",
    "    :return: Precision value as a float.\n",
    "    \"\"\"\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Convert ground truth data into a dictionary for quick lookup by ID\n",
    "    ground_truth_dict = {entry['id']: entry['triples'] for entry in ground_truth}\n",
    "    \n",
    "    # Loop through each system-predicted entry\n",
    "    for entry in system_predicted:\n",
    "        predicted_id = entry['id']\n",
    "        predicted_triples = entry.get('triples', [])\n",
    "        \n",
    "        if predicted_id in ground_truth_dict:\n",
    "            ground_truth_triples = ground_truth_dict[predicted_id]\n",
    "            \n",
    "            # Normalize the ground truth triples\n",
    "            normalized_ground_truth = set(\n",
    "                normalize_triple(triple['sub'], triple['rel'], triple['obj']) \n",
    "                for triple in ground_truth_triples\n",
    "            )\n",
    "            \n",
    "            # Normalize the predicted triples\n",
    "            normalized_predictions = set(\n",
    "                normalize_triple(triple['sub'], triple['rel'], triple['obj']) \n",
    "                for triple in predicted_triples\n",
    "            )\n",
    "            \n",
    "            # Count correctly predicted triples\n",
    "            correct_predictions += len(normalized_predictions & normalized_ground_truth)\n",
    "        \n",
    "        # Update total predictions\n",
    "        total_predictions += len(predicted_triples)\n",
    "    \n",
    "    # Calculate precision\n",
    "    precision = correct_predictions / total_predictions if total_predictions > 0 else 0.0\n",
    "    \n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "4f97bcfb-ccdb-4eee-9bf7-5284f6560eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(ground_truth_file, system_predicted_file):\n",
    "    # Load data from JSONL files\n",
    "    ground_truth = load_jsonl(ground_truth_file)\n",
    "    system_predicted = load_jsonl(system_predicted_file)\n",
    "    \n",
    "    # Calculate precision\n",
    "    precision = calculate_precision(ground_truth, system_predicted)\n",
    "    \n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "# Example usage:\n",
    "# main('ground_truth.jsonl', 'LLM_Response.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "4c20cd12-c8cc-4c74-9078-cd6ad3c8a818",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'triples'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[241], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mont_1_movie_prompts.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLLM_Response.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[240], line 7\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(ground_truth_file, system_predicted_file)\u001b[0m\n\u001b[1;32m      4\u001b[0m system_predicted \u001b[38;5;241m=\u001b[39m load_jsonl(system_predicted_file)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate precision\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m precision \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_predicted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[239], line 13\u001b[0m, in \u001b[0;36mcalculate_precision\u001b[0;34m(ground_truth, system_predicted)\u001b[0m\n\u001b[1;32m     10\u001b[0m total_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Convert ground truth data into a dictionary for quick lookup by ID\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m ground_truth_dict \u001b[38;5;241m=\u001b[39m {entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]: \u001b[43mentry\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtriples\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m ground_truth}\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Loop through each system-predicted entry\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m system_predicted:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'triples'"
     ]
    }
   ],
   "source": [
    "main(('ont_1_movie_prompts.jsonl'), 'LLM_Response.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be345c4b-636d-4a87-bbb5-af4430f1a2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca16303-1a04-4436-bbcb-83493d87277f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfb6e44-813e-46a1-8111-a10a71300c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "db0fe1d9-13e7-48fc-84bc-59e92f4363f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall_f1(gold: set, pred: set) -> (float, float, float):\n",
    "    \"\"\"\n",
    "    Method to calculate precision, recall and f1:\n",
    "        Precision is calculated as correct_triples/predicted_triples and\n",
    "        Recall as correct_triples/gold_triples\n",
    "        F1 as the harmonic mean of precision and recall.\n",
    "    :param gold: items in the gold standard\n",
    "    :param pred: items in the system prediction\n",
    "    :return:\n",
    "        p: float - precision\n",
    "        r: float - recall\n",
    "        f1: float - F1\n",
    "    \"\"\"\n",
    "    if not pred:\n",
    "        # If there are no predictions, precision and F1 are 0\n",
    "        return 0, 0, 0\n",
    "\n",
    "    # Calculate precision\n",
    "    correct = len(gold.intersection(pred))\n",
    "    p = correct / len(pred) if len(pred) > 0 else 0\n",
    "\n",
    "    if not gold:\n",
    "        # If there are no gold triples, recall and F1 are 0\n",
    "        return p, 0, 0\n",
    "\n",
    "    # Calculate recall\n",
    "    r = correct / len(gold) if len(gold) > 0 else 0\n",
    "\n",
    "    # Calculate F1 score\n",
    "    if p + r > 0:\n",
    "        f1 = 2 * (p * r) / (p + r)\n",
    "    else:\n",
    "        f1 = 0\n",
    "\n",
    "    return p, r, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "3225741d-5f2c-413f-85e9-a3ef45025865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(ground_truth_file, system_predicted_file):\n",
    "    # Load the ground truth and system predicted data\n",
    "    ground_truth = load_jsonl(ground_truth_file)\n",
    "\n",
    "    system_predicted = load_jsonl(system_predicted_file)\n",
    "    \n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision, recall, f1 = calculate_precision_recall_f1(\n",
    "        extract_normalized_triples(ground_truth),\n",
    "         extract_normalized_triples(system_predicted)\n",
    "    )\n",
    "    \n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "def extract_normalized_triples(data):\n",
    "    \"\"\"\n",
    "    Extract and normalize triples from the dataset.\n",
    "    :param data: List of dictionaries containing triples.\n",
    "    :return: A set of normalized triples.\n",
    "    \"\"\"\n",
    "    normalized_triples = set()\n",
    "    for entry in data:\n",
    "        for triple in entry.get('triples', []):\n",
    "            norm_triple = normalize_triple(triple['sub'], triple['rel'], triple['obj'])\n",
    "            normalized_triples.add(norm_triple)\n",
    "    return normalized_triples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "1e564841-eafc-482b-a821-3e0897d2d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main('ont_1_movie_prompts.jsonl', 'LLM_Response.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e2a00b-48b4-42f1-a20f-41936cc58109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd156c2-ad78-4ba7-b8c0-bc6a8446dc34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48211a15-e1ab-4dc6-a2ed-327524177885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807934a9-5f32-4246-928e-9ffbc0d433a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc77c1bc-89cf-4589-9077-d49cea483d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "d09c46cc-4e90-4506-8d7d-51d4ca694f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: [{'generated_text': \"Hey how are you doing today? I'm doing great! I'm so happy that you have decided to join me on my journey to being more healthy and happy. This is a journey that I will be taking with you for the next 21\"}]\n",
      "Execution Time: 3.08 seconds\n"
     ]
    }
   ],
   "source": [
    "# Record the start time\n",
    "start_time = time.time()\n",
    "# Execute the pipeline\n",
    "\n",
    "result = text_generator(\"Hey how are you doing today?\", max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the duration\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the result and execution time\n",
    "print(\"Result:\", result)\n",
    "print(f\"Execution Time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "7615b3b7-6fe5-48bc-b614-5aa003983418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first prompt\n",
    "item = prompts_data[0]\n",
    "prompt_id = item['id']\n",
    "prompt_text = item['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "90c52733-4f63-44c2-a05c-c34f91b595c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '\\nGiven the following ontology and sentences, please extract the triples from the sentence according to the relations in the ontology. In the output, only include the triples in the given output format.\\nCONTEXT:\\nOntology Concepts: human, city, country, film, film genre, genre, film production company, film award, award, written work, film character, film organization,\\nOntology Relations: director(film,human), screenwriter(film,human), genre(film,genre), based_on(film,written work), cast_member(film,human), award_received(film,award), production_company(film,film production company), country_of_origin(film,country), publication_date(film,), characters(film,film character), narrative_location(film,city), filming_location(film,city), main_subject(film,), nominated_for(film,award), cost(film,)\\n\\nExample Sentence: Resident Evil: Damnation, known as Biohazard: Damnation ( , BaiohazÄ\\x81do: DamunÄ\\x93shon) in Japan, is a 2012 Japanese adult animated biopunk horror action film by Capcom and Sony Pictures Entertainment Japan, directed by Makoto Kamiya and produced by Hiroyuki Kobayashi.\\nExample Output: director(Resident Evil: Damnation,Makoto Kamiya)\\n\\nTest Sentence: Bleach: Hell Verse (Japanese: BLEACH , Hepburn: BurÄ«chi Jigoku-Hen) is a 2010 Japanese animated film directed by Noriyuki Abe.\\nTest Output:  director(Bleach: Hell Verse,Noriyuki Abe)\\n'}]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate output using the Llama model\n",
    "response = text_generator(prompt_text, max_length=500, num_return_sequences=1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6eae7d-0cc1-4aa3-8234-98b599a0ec22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
